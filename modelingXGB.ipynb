{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "\n",
    "from prepare_flight_data import *\n",
    "from feature_engineering import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATOP</th>\n",
       "      <th>FLTID</th>\n",
       "      <th>DEPSTN</th>\n",
       "      <th>ARRSTN</th>\n",
       "      <th>STD</th>\n",
       "      <th>STA</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>AC</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>icao_ARR</th>\n",
       "      <th>iata_ARR</th>\n",
       "      <th>name_ARR</th>\n",
       "      <th>city_ARR</th>\n",
       "      <th>subd_ARR</th>\n",
       "      <th>country_ARR</th>\n",
       "      <th>elevation_ARR</th>\n",
       "      <th>lat_ARR</th>\n",
       "      <th>lon_ARR</th>\n",
       "      <th>tz_ARR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_15674</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>TU 0564</td>\n",
       "      <td>NKC</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 00:15:00</td>\n",
       "      <td>2016-01-01 04:30:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 320IMV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DTTA</td>\n",
       "      <td>TUN</td>\n",
       "      <td>Tunis Carthage International Airport</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>TN</td>\n",
       "      <td>22</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>10.22720</td>\n",
       "      <td>Africa/Tunis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_15676</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>TU 0714</td>\n",
       "      <td>JED</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 00:55:00</td>\n",
       "      <td>2016-01-01 05:30:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 332IFM</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DTTA</td>\n",
       "      <td>TUN</td>\n",
       "      <td>Tunis Carthage International Airport</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>TN</td>\n",
       "      <td>22</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>10.22720</td>\n",
       "      <td>Africa/Tunis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_15675</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>TU 0614</td>\n",
       "      <td>DKR</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 01:20:00</td>\n",
       "      <td>2016-01-01 05:55:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 320IMU</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DTTA</td>\n",
       "      <td>TUN</td>\n",
       "      <td>Tunis Carthage International Airport</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>Tunis</td>\n",
       "      <td>TN</td>\n",
       "      <td>22</td>\n",
       "      <td>36.851002</td>\n",
       "      <td>10.22720</td>\n",
       "      <td>Africa/Tunis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_id_30980</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>UG 0002</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>2016-01-01 06:15:00</td>\n",
       "      <td>2016-01-01 07:15:00</td>\n",
       "      <td>SCH</td>\n",
       "      <td>UG AT7LBD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>DTTJ</td>\n",
       "      <td>DJE</td>\n",
       "      <td>Djerba Zarzis International Airport</td>\n",
       "      <td>Djerba</td>\n",
       "      <td>Madanin</td>\n",
       "      <td>TN</td>\n",
       "      <td>19</td>\n",
       "      <td>33.875000</td>\n",
       "      <td>10.77550</td>\n",
       "      <td>Africa/Tunis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_id_7179</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>TU 0880</td>\n",
       "      <td>TUN</td>\n",
       "      <td>AMS</td>\n",
       "      <td>2016-01-01 06:30:00</td>\n",
       "      <td>2016-01-01 09:20:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 736IOP</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EHAM</td>\n",
       "      <td>AMS</td>\n",
       "      <td>Amsterdam Airport Schiphol</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>North Holland</td>\n",
       "      <td>NL</td>\n",
       "      <td>-11</td>\n",
       "      <td>52.308601</td>\n",
       "      <td>4.76389</td>\n",
       "      <td>Europe/Amsterdam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      DATOP     FLTID DEPSTN ARRSTN                 STD  \\\n",
       "0  train_id_15674 2016-01-01  TU 0564     NKC    TUN 2016-01-01 00:15:00   \n",
       "1  train_id_15676 2016-01-01  TU 0714     JED    TUN 2016-01-01 00:55:00   \n",
       "2  train_id_15675 2016-01-01  TU 0614     DKR    TUN 2016-01-01 01:20:00   \n",
       "3  train_id_30980 2016-01-01  UG 0002     TUN    DJE 2016-01-01 06:15:00   \n",
       "4   train_id_7179 2016-01-01  TU 0880     TUN    AMS 2016-01-01 06:30:00   \n",
       "\n",
       "                  STA STATUS         AC  target  ... icao_ARR iata_ARR  \\\n",
       "0 2016-01-01 04:30:00    ATA  TU 320IMV     0.0  ...     DTTA      TUN   \n",
       "1 2016-01-01 05:30:00    ATA  TU 332IFM   195.0  ...     DTTA      TUN   \n",
       "2 2016-01-01 05:55:00    ATA  TU 320IMU    49.0  ...     DTTA      TUN   \n",
       "3 2016-01-01 07:15:00    SCH  UG AT7LBD     0.0  ...     DTTJ      DJE   \n",
       "4 2016-01-01 09:20:00    ATA  TU 736IOP    36.0  ...     EHAM      AMS   \n",
       "\n",
       "                               name_ARR   city_ARR       subd_ARR country_ARR  \\\n",
       "0  Tunis Carthage International Airport      Tunis          Tunis          TN   \n",
       "1  Tunis Carthage International Airport      Tunis          Tunis          TN   \n",
       "2  Tunis Carthage International Airport      Tunis          Tunis          TN   \n",
       "3   Djerba Zarzis International Airport     Djerba        Madanin          TN   \n",
       "4            Amsterdam Airport Schiphol  Amsterdam  North Holland          NL   \n",
       "\n",
       "   elevation_ARR    lat_ARR   lon_ARR            tz_ARR  \n",
       "0             22  36.851002  10.22720      Africa/Tunis  \n",
       "1             22  36.851002  10.22720      Africa/Tunis  \n",
       "2             22  36.851002  10.22720      Africa/Tunis  \n",
       "3             19  33.875000  10.77550      Africa/Tunis  \n",
       "4            -11  52.308601   4.76389  Europe/Amsterdam  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, df_test = load_prepare_flight_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering\n",
    "\n",
    "Runs around 15 seconds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances\n",
    "df = lat_lon_distance(df)\n",
    "df_test = lat_lon_distance(df_test)\n",
    "# Initialize custom transformer\n",
    "#features_enable = [1, 1, 1, 1, 1] # [domestic, dep_hour, dep_weekday, duration_min, operator]\n",
    "# Pipeline to add features\n",
    "attr_addr = flight_preprocessor()\n",
    "df = attr_addr.fit_transform(df)\n",
    "df_test = attr_addr.transform(df_test)\n",
    "# Store ID for submission\n",
    "sub_id = df_test.ID\n",
    "# Drop unimportant columns\n",
    "cols_to_drop = [\"icao_DEP\", \"iata_DEP\", \"name_DEP\", \"subd_DEP\", \"city_DEP\", \n",
    "                \"icao_ARR\", \"iata_ARR\", \"name_ARR\", \"subd_ARR\", \"city_ARR\", \n",
    "                \"ID\", 'DATOP', \"STA\", \"STD\", 'tz_DEP', 'tz_ARR']\n",
    "df = drop_column(df, cols_to_drop)\n",
    "df_test = drop_column(df_test, cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FLTID', 'DEPSTN', 'ARRSTN', 'STATUS', 'AC', 'target', 'country_DEP',\n",
       "       'elevation_DEP', 'lat_DEP', 'lon_DEP', 'country_ARR', 'elevation_ARR',\n",
       "       'lat_ARR', 'lon_ARR', 'distance', 'domestic', 'dep_hour', 'dep_weekday',\n",
       "       'duration_min', 'operator', 'arr_hour', 'dep_day', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  (97049,) \n",
      "Test size:  (10784,)\n"
     ]
    }
   ],
   "source": [
    "# Train dataset\n",
    "X = df.copy()\n",
    "y = np.log(X.pop(\"target\")+1)\n",
    "# Test dataset for submission\n",
    "X_submission = df_test\n",
    "# Test-train-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=RSEED)\n",
    "print(\"Train size: \", y_train.shape, \"\\n\" \"Test size: \", y_test.shape)\n",
    "# Remove outliers to improve zindi score\n",
    "X_train = X_train[y_train < y_train.quantile(q=0.99)]\n",
    "y_train = y_train[y_train < y_train.quantile(q=0.99)]\n",
    "# X_test = X_test[y_test < y_train.quantile(q=0.99)]\n",
    "# y_test = y_test[y_test < y_train.quantile(q=0.99)]\n",
    "# print(\"Train size: \", y_train.shape, \"\\n\" \"Test size: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_country_ARR = [np.asarray(df.groupby(\"country_ARR\").mean().sort_values(by=\"target\").index)]\n",
    "categories_country_DEP = [np.asarray(df.groupby(\"country_DEP\").mean().sort_values(by=\"target\").index)]\n",
    "categories_DEPSTN = [np.asarray(df.groupby(\"DEPSTN\").mean().sort_values(by=\"target\").index)]\n",
    "categories_ARRSTN = [np.asarray(df.groupby(\"ARRSTN\").mean().sort_values(by=\"target\").index)]\n",
    "categories_FLTID = [np.asarray(df.groupby(\"FLTID\").mean().sort_values(by=\"target\").index)]\n",
    "categories_AC = [np.asarray(df.groupby(\"AC\").mean().sort_values(by=\"target\").index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor pipelines\n",
    "num_cols = ['distance', 'domestic', 'dep_hour', 'dep_weekday',\n",
    "            'duration_min', \"dep_day\", \"arr_hour\"]\n",
    "cat_cols = [\"STATUS\", \"operator\"]\n",
    "# Preprocessor for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    #('num_scaler', StandardScaler()),\n",
    "    ('num_scaler', MinMaxScaler())\n",
    "])\n",
    "# Preprocessor for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Put together preprocessor pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols),\n",
    "    ('cat_AC', OrdinalEncoder(categories=categories_AC, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_AC[0])+1)), [\"AC\"]),\n",
    "    ('cat_FLTID', OrdinalEncoder(categories=categories_FLTID, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_FLTID[0])+1)), [\"FLTID\"]),\n",
    "    ('cat_ARRSTN', OrdinalEncoder(categories=categories_ARRSTN, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_ARRSTN[0])+1)), [\"ARRSTN\"]),\n",
    "    ('cat_DEPSTN', OrdinalEncoder(categories=categories_DEPSTN, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_DEPSTN[0])+1)), [\"DEPSTN\"]),\n",
    "    ('cat_country_ARR', OrdinalEncoder(categories=categories_country_ARR, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_country_ARR[0])+1)), [\"country_ARR\"]),\n",
    "    ('cat_country_DEP', OrdinalEncoder(categories=categories_country_DEP, handle_unknown=\"use_encoded_value\", \n",
    "                                unknown_value=(len(categories_country_DEP[0])+1)), [\"country_DEP\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xgb = Pipeline([\n",
    "    # ('attr_addr', features_pipeline),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=6,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               Pipeline(steps=[('num_scaler',\n",
       "                                                                                                MinMaxScaler())]),\n",
       "                                                                               ['distance',\n",
       "                                                                                'domestic',\n",
       "                                                                                'dep_hour',\n",
       "                                                                                'dep_weekday',\n",
       "                                                                                'duration_min',\n",
       "                                                                                'dep_day',\n",
       "                                                                                'arr_hour']),\n",
       "                                                                              ('cat',\n",
       "                                                                               Pipeline(steps=[('cat_encoder',\n",
       "                                                                                                OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                               ['STATUS',\n",
       "                                                                                'operator']),\n",
       "                                                                              ('cat_AC...\n",
       "                                                           random_state=None,\n",
       "                                                           reg_alpha=None,\n",
       "                                                           reg_lambda=None,\n",
       "                                                           scale_pos_weight=None,\n",
       "                                                           subsample=None,\n",
       "                                                           tree_method=None,\n",
       "                                                           validate_parameters=None,\n",
       "                                                           verbosity=None))]),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'xgb__eta': [0.05],\n",
       "                                        'xgb__gamma': [1, 2, 3],\n",
       "                                        'xgb__learning_rate': [0.1],\n",
       "                                        'xgb__max_depth': [10],\n",
       "                                        'xgb__n_estimators': [200],\n",
       "                                        'xgb__subsample': [0.5]},\n",
       "                   scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining parameter grid (as dictionary)\n",
    "param_grid = {'xgb__n_estimators': list(range(10,100,50)),\n",
    "              'xgb__learning_rate': list(np.linspace(0.001, 0.5, 100)),\n",
    "              'xgb__subsample': [0.5],\n",
    "              'xgb__gamma': [5],\n",
    "              'xgb__max_depth': list(range(1, 10))\n",
    "    }\n",
    "# Final parameters\n",
    "param_grid = {'xgb__n_estimators': [200],\n",
    "              'xgb__learning_rate': [0.1],\n",
    "              'xgb__subsample': [0.5],\n",
    "              'xgb__gamma': [1, 2, 3],\n",
    "              'xgb__max_depth': [10],\n",
    "              'xgb__eta': [0.05]\n",
    "    }\n",
    "# Instantiate gridsearch and define the metric to optimize \n",
    "gs = RandomizedSearchCV(pipe_xgb,  param_grid, scoring='neg_root_mean_squared_error',\n",
    "                  cv=6, verbose=0, n_jobs=-1, n_iter=20)\n",
    "\n",
    "# Fit gridsearch object to data.\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create linear targets and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for test dataset 132.26709521647086\n"
     ]
    }
   ],
   "source": [
    "y_train_lin = np.exp(y_train) - 1\n",
    "bl = y_train_lin.median()\n",
    "y_pred_bl = [bl for el in X_test.index]\n",
    "y_test_lin = np.exp(y_test) - 1\n",
    "score = mean_squared_error(y_test_lin, y_pred_bl, squared=False)\n",
    "print(\"RMSE for test dataset\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output best estimator and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('num_scaler',\n",
      "                                                                   MinMaxScaler())]),\n",
      "                                                  ['distance', 'domestic',\n",
      "                                                   'dep_hour', 'dep_weekday',\n",
      "                                                   'duration_min', 'dep_day',\n",
      "                                                   'arr_hour']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('cat_encoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['STATUS', 'operator']),\n",
      "                                                 ('cat_AC',\n",
      "                                                  OrdinalEncoder(categories=[array...\n",
      "                              eta=0.05, gamma=2, gpu_id=-1,\n",
      "                              importance_type=None, interaction_constraints='',\n",
      "                              learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "                              min_child_weight=1, missing=nan,\n",
      "                              monotone_constraints='()', n_estimators=200,\n",
      "                              n_jobs=1, num_parallel_tree=1, predictor='auto',\n",
      "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "                              scale_pos_weight=1, subsample=0.5,\n",
      "                              tree_method='exact', validate_parameters=1,\n",
      "                              verbosity=None))])\n",
      "RMSE for train dataset 59.758073171308766\n",
      "RMSE for test dataset 123.67010316922895\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_estimator_)\n",
    "\n",
    "# Making predictions on the train dataset\n",
    "y_pred = gs.predict(X_train)\n",
    "y_pred_lin = np.exp(y_pred) - 1\n",
    "y_train_lin = np.exp(y_train) - 1\n",
    "#y_pred = best_model.predict(X_train)\n",
    "score = mean_squared_error(y_train_lin, y_pred_lin, squared=False)\n",
    "print(\"RMSE for train dataset\", score)\n",
    "\n",
    "# Making predictions on the test dataset\n",
    "y_pred = gs.predict(X_test)\n",
    "y_pred_lin = np.exp(y_pred) - 1\n",
    "y_test_lin = np.exp(y_test) - 1\n",
    "score = mean_squared_error(y_test_lin, y_pred_lin, squared=False)\n",
    "print(\"RMSE for test dataset\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export predictions for error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test = pd.DataFrame(X_test)\n",
    "df_y_test = pd.DataFrame(y_test)\n",
    "df_y_pred = pd.DataFrame(y_pred)\n",
    "df_X_test.to_csv(\"data/rf_X_test.csv\", index=False)\n",
    "df_y_test.to_csv(\"data/rf_y_test.csv\", index=False)\n",
    "df_y_pred.to_csv(\"data/rf_y_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export submission for zindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_target = np.exp(gs.predict(X_submission)) - 1\n",
    "\n",
    "submission = pd.DataFrame(sub_id)\n",
    "submission[\"target\"] = sub_target\n",
    "submission.to_csv(\"data/submission_rf1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/xgb_model.sav'\n",
    "pickle.dump(gs.best_estimator_, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9585d3ff2bc676d4c3541448061c25f425c3fbc2797c0d4ea2cbc5687c5cfda7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
